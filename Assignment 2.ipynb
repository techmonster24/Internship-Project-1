{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10\n",
    "jobs data.\n",
    "\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/ 2. Enter “Data Analyst” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the drivers and URL\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "url=(\"https://www.naukri.com/\")\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the Function Definition\n",
    "\n",
    "#Let's enter the details in the search column\n",
    "search_field_designation=driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "search_field_location=driver.find_element_by_id(\"qsb-location-sugg\")\n",
    "\n",
    "#Let's enter the job title and location \n",
    "search_field_designation.send_keys(\"Data Analyst\")\n",
    "search_field_location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create empty list for storing the data\n",
    "\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience_required=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the job tile\n",
    "\n",
    "titles=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in titles:\n",
    "    job_title.append(i.text)\n",
    "job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the job location\n",
    "\n",
    "location=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "for i in location:\n",
    "    job_location.append(i.text)\n",
    "job_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the company name\n",
    "\n",
    "companies=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in companies:\n",
    "    company_name.append(i.text)\n",
    "company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the exoerience required\n",
    "\n",
    "experience=driver.find_elements_by_xpath('//span[@class=\"ellipsis fleft fs12 lh16\"]')\n",
    "for i in experience:\n",
    "    experience_required.append(i.text)\n",
    "experience_required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a DataFrame for our data\n",
    "\n",
    "data_analyst_jobs=pd.DataFrame({})\n",
    "data_analyst_jobs['Job Title']=job_title[0:10]    \n",
    "data_analyst_jobs['Job Location']=job_location[0:10]\n",
    "data_analyst_jobs['Company Name']=company_name[0:10]\n",
    "data_analyst_jobs['Experience Required']=experience_required[0:10]\n",
    "data_analyst_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You\n",
    "have to scrape the job-title, job-location, company_name. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. First get the webpage https://www.naukri.com/ 2. Enter “Data Scientist” in “Skill, Designations, Companies” field and enter “Bangalore” in “enter the\n",
    "location” field.\n",
    "\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the drivers and URL\n",
    "\n",
    "driver = webdriver.Chrome('chromedriver.exe')\n",
    "url=(\"https://www.naukri.com/\")\n",
    "driver.get(url)\n",
    "driver.implicitly_wait(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the Function Definition\n",
    "\n",
    "#Let's navigate to search column\n",
    "search_field_designation=driver.find_element_by_id(\"qsb-keyword-sugg\")\n",
    "search_field_location=driver.find_element_by_id(\"qsb-location-sugg\")\n",
    "\n",
    "#Let's enter the job title and location\n",
    "search_field_designation.send_keys(\"Data Scientist\")\n",
    "search_field_location.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_button=driver.find_element_by_xpath(\"//div[@class='search-btn']/button\")\n",
    "search_button.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create empty list for storing the data\n",
    "\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "job_description=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the job tile\n",
    "\n",
    "titles=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "for i in titles:\n",
    "    job_title.append(i.text)\n",
    "job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the job location\n",
    "\n",
    "location=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']/span\")\n",
    "for i in location:\n",
    "    job_location.append(i.text)\n",
    "job_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the company name\n",
    "\n",
    "companies=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "for i in companies:\n",
    "    company_name.append(i.text)\n",
    "company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's scrape the detail job description\n",
    "\n",
    "from selenium.common.exceptions import  NoSuchElementException\n",
    "urls=[]\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\"):\n",
    "    urls.append(i.get_attribute(\"href\"))\n",
    "\n",
    "for url in urls[0:10]:\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        description=driver.find_element_by_xpath(\"//section[@class='job-desc']\").text\n",
    "        job_description.append(description)\n",
    "    except NoSuchElementException :\n",
    "        job_description.append(\"Not Available\")\n",
    "job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create a DataFrame for our data\n",
    "\n",
    "data_scientist_jobs=pd.DataFrame({})\n",
    "data_scientist_jobs['Job Title']=job_title[0:10]    \n",
    "data_scientist_jobs['Job Location']=job_location[0:10]\n",
    "data_scientist_jobs['Company Name']=company_name[0:10]\n",
    "data_scientist_jobs['Detailed Job Description']=job_description[0:10]\n",
    "data_scientist_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question 3: In this question you have to scrape data using the filters available on the webpage\n",
    "You have to use the location and salary filter. You have to scrape data for “Data Scientist” designation for first 10 job results. You have to scrape the job-title, job-location, company_name, experience_required. The location filter to be used is “Delhi/NCR” The salary filter to be used is “3-6” lakhs The task will be done as shown in the below steps:\n",
    "\n",
    "First get the webpage https://www.naukri.com/\n",
    "Enter “Data Scientist” in “Skill,Designations,Companies” field.\n",
    "Then click the search button.\n",
    "Then apply the location filter and salary filter by checking the respective boxes\n",
    "Then scrape the data for the first 10 jobs results you get.\n",
    "Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's call the Function Definition\n",
    "\n",
    "def jobs_data_Scientist(url):\n",
    "    driver = webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Let's create empty list for storing the data\n",
    "    ds_job_title=[]\n",
    "    ds_company_name=[]\n",
    "    ds_location=[]\n",
    "    ds_experience=[]\n",
    "    ds_salary=[]\n",
    "    \n",
    "    #Let's navigate to the search column\n",
    "    driver.find_element_by_xpath('//input[@class=\"sugInp\"]').send_keys('Data Scientist')\n",
    "    driver.find_element_by_xpath('//button[@class=\"btn\"]').click()\n",
    "    time.sleep(10)\n",
    "    \n",
    "    # Let's create filter for job location\n",
    "    driver.find_element_by_xpath('//span[@title=\"Delhi/NCR\"]').click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Let's create filter for salary\n",
    "    driver.find_element_by_xpath('//span[@title=\"3-6 Lakhs\"]').click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Let's scrape the job tile\n",
    "    job_titles=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "    for i in job_titles:\n",
    "        ds_job_title.append(i.text)\n",
    "    \n",
    "    # Let's scrape the company name\n",
    "    company_name=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "    for i in company_name:\n",
    "        ds_company_name.append(i.text)\n",
    "    \n",
    "    # Let's scrape the company location    \n",
    "    location=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]')\n",
    "    for i in location:\n",
    "        ds_location.append(i.text)\n",
    "    \n",
    "        \n",
    "    # Let's scrape the experience\n",
    "    experience=driver.find_elements_by_xpath('//span[@class=\"ellipsis fleft fs12 lh16\"]')\n",
    "    for i in experience:\n",
    "        ds_experience.append(i.text)\n",
    "    \n",
    "    # Let's scrape the salary\n",
    "    salary=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi salary\"]')\n",
    "    for i in salary:\n",
    "        ds_salary.append(i.text)\n",
    "    \n",
    "    # Let's create a DataFrame for our data\n",
    "    ds_jobs=pd.DataFrame({})\n",
    "    ds_jobs['Job Title']=ds_job_title[0:10]\n",
    "    ds_jobs['Company Name']=ds_company_name[0:10]\n",
    "    ds_jobs['Job Location']=ds_location[0:10]\n",
    "    ds_jobs['Experience']=ds_experience[0:10]\n",
    "    return ds_jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the Calling Function\n",
    "\n",
    "jobs_data_Scientist('https://www.naukri.com')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4: Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's call the Function Definition\n",
    "\n",
    "def flipkart_sunglasses(url):\n",
    "    \n",
    "    # Let's load the drivers and URL\n",
    "    driver = webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    #Let's navigate to search bar\n",
    "    driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #Let's search for sunglasses product and then click search button\n",
    "    driver.find_element_by_xpath('//input[@title=\"Search for products, brands and more\"]').send_keys('sunglasses')\n",
    "    driver.find_element_by_xpath('//button[@type=\"submit\"]').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Let's create empty list for storing the data\n",
    "    sunglass_brand_name=[]\n",
    "    sunglass_description=[]\n",
    "    sunglass_price=[]\n",
    "    sunglass_discount=[]\n",
    "    \n",
    "    # Let's scrape the data\n",
    "    j=0\n",
    "    while j<3:\n",
    "        time.sleep(3)\n",
    "        brands=driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "        for brand in brands:\n",
    "            sunglass_brand_name.append(brand.text)\n",
    "        \n",
    "        descriptions=driver.find_elements_by_xpath('//a[contains(@class,\"IRpwTa\")]')\n",
    "        for description in descriptions:\n",
    "            sunglass_description.append(description.text)\n",
    "        \n",
    "        prices=driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "        for price in prices:\n",
    "            sunglass_price.append(price.text)\n",
    "        \n",
    "        \n",
    "    # Let's create a DataFrame for our data   \n",
    "    sunglasses_flip=pd.DataFrame({})\n",
    "    sunglasses_flip['Product Brand']=sunglass_brand_name[:100]\n",
    "    sunglasses_flip['Product Description']=sunglass_description[:100]\n",
    "    sunglasses_flip['Price of Product']=sunglass_price[:100]\n",
    "    \n",
    "    return sunglasses_flip\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the Calling Function\n",
    "\n",
    "flipkart_sunglasses('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5: Scrape 100 reviews data from flipkart.com for iphone11 phone. You have to go the link:\n",
    "\n",
    "https://www.flipkart.com/apple-iphone-11-black-64-gb-includes- earpods-power-\n",
    "adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the above page you have to scrape the tick marked attributes. These are:\n",
    "1. Rating\n",
    "2. Review_summary\n",
    "3. Full review\n",
    "4. You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's call the Function Definition\n",
    "\n",
    "def iphone_100_review(url):\n",
    "    \n",
    "    # Let's load the drivers and URL\n",
    "    driver = webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Let's create empty list for storing the data\n",
    "    rating=[]\n",
    "    review_summary=[]\n",
    "    full_review=[]\n",
    "    \n",
    "    # Let's scrape the data\n",
    "    j=0\n",
    "    while j<10:\n",
    "    \n",
    "        ratings=driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "        for i in ratings:\n",
    "            rating.append(i.text)\n",
    "        \n",
    "        reviews=driver.find_elements_by_xpath('//p[@class=\"_2-N8zT\"]')\n",
    "        for i in reviews:\n",
    "            review_summary.append(i.text)\n",
    "        \n",
    "        full_reviews=driver.find_elements_by_xpath('//div[@class=\"t-ZTKy\"]')\n",
    "        for i in full_reviews:\n",
    "            full_review.append(i.text)\n",
    "        time.sleep(10)    \n",
    "        \n",
    "        driver.find_element_by_xpath('//a[@class=\"_1LKTO3\"]').click()\n",
    "        time.sleep(10)\n",
    "        j+=1\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # Let's create a DataFrame for our data\n",
    "    iphone_review=pd.DataFrame({})\n",
    "    iphone_review['Rating']=rating\n",
    "    iphone_review['Review Summary']=review_summary\n",
    "    iphone_review['Full Reviews']=full_review\n",
    "    return iphone_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the Calling Function\n",
    "\n",
    "iphone_100_review('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGR3QP11A&marketplace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the\n",
    "search field.\n",
    "You have to scrape 4 attributes of each sneaker:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's call the Function Definition\n",
    "\n",
    "def flip_sneakers(url):\n",
    "    \n",
    "    # Let's load the drivers and URL\n",
    "    driver = webdriver.Chrome('chromedriver.exe')\n",
    "    driver.get(url)\n",
    "    \n",
    "    #Let's navigate to search bar\n",
    "    driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #Let's search for sneaker product and then click search button\n",
    "    driver.find_element_by_xpath('//input[@title=\"Search for products, brands and more\"]').send_keys('sneakers')\n",
    "    driver.find_element_by_xpath('//button[@type=\"submit\"]').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Let's create empty list for storing the data\n",
    "    sneaker_brand_name=[]\n",
    "    sneaker_description=[]\n",
    "    sneaker_price=[]\n",
    "    sneaker_discount=[]\n",
    "    \n",
    "    # Let's scrape the data\n",
    "    j=0\n",
    "    while j<3:\n",
    "        time.sleep(3)\n",
    "        brands=driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "        for brand in brands:\n",
    "            sneaker_brand_name.append(brand.text)\n",
    "        \n",
    "        descriptions=driver.find_elements_by_xpath('//a[contains(@class,\"IRpwTa\")]')\n",
    "        for description in descriptions:\n",
    "            sneaker_description.append(description.text)\n",
    "        \n",
    "        prices=driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "        for price in prices:\n",
    "            sneaker_price.append(price.text)\n",
    "        \n",
    "        j+=1\n",
    "        \n",
    "    # Let's create a DataFrame for our data   \n",
    "    sneakers_flipkart=pd.DataFrame({})\n",
    "    sneakers_flipkart['Brand Name']=sneaker_brand_name[:100]\n",
    "    sneakers_flipkart['Product Descriptions']=sneaker_description[:100]\n",
    "    sneakers_flipkart['Price']=sneaker_price[:100]\n",
    "    sneakers_flipkart['Discount %']=sneaker_discount[:100]\n",
    "    return sneakers_flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
